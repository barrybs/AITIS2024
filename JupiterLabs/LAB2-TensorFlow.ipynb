{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qnyTxjK_GbOD"
   },
   "source": [
    "# LAB 2 - MLP per problemi di riconoscimento immagini\n",
    "I problemi di computer vision in generale sono molto più complessi rispetto alla mera previsione di un valore data una breve successione di numeri reali.\n",
    "\n",
    "In questo laboratorio guidato vedremo come una rete neurale può essere addestrata a risolvere un problema di classificazione multiclasse (>2 classi). Le classi sono 10 e dovremo addestare la rete a riconoscere immagini di abbigliamento utilizzando un dataset con 10 classi.\n",
    "\n",
    "Il dataset utilizzato è il [Fashion MNIST dataset](https://github.com/zalandoresearch/fashion-mnist)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H41FYgtlHPjW"
   },
   "source": [
    "## Codice\n",
    "\n",
    "Importiamo le librerie che ci servono: \n",
    "* tensorFlow per creare e addestare il modello.\n",
    "* numpy per gestire facilmente array e matrici.\n",
    "* matplotlib per disegnare agilmente grafici dei dati e dei risultati.\n",
    "* time per misurare i tempi di elaborazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "q3KzJyjv3rnA"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n_n1U5do3u_F"
   },
   "source": [
    "## Fashion MNIST Dataset\n",
    "\n",
    "Il [Fashion MNIST dataset](https://github.com/zalandoresearch/fashion-mnist) è una collezione di immagini in scala di grigi con dimensioni di 28x28 pixel, raffiguranti capi di abbigliamento. Ogni immagine è associata a un'etichetta, come illustrato nella seguente tabella:\n",
    "\n",
    "| Etichetta | Descrizione         |\n",
    "| --------- | ------------------- |\n",
    "| 0         | Maglietta/top       |\n",
    "| 1         | Pantaloni           |\n",
    "| 2         | Pullover            |\n",
    "| 3         | Vestito             |\n",
    "| 4         | Cappotto            |\n",
    "| 5         | Sandali             |\n",
    "| 6         | Camicia             |\n",
    "| 7         | Scarpe da ginnastica |\n",
    "| 8         | Borsa               |\n",
    "| 9         | Stivali alla caviglia |\n",
    "\n",
    "Questo dataset è disponibile direttamente nell'API di [tf.keras.datasets](https://www.tensorflow.org/api_docs/python/tf/keras/datasets) e può essere caricato in questo modo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "PmxkHFpt31bM"
   },
   "outputs": [],
   "source": [
    "# Caricamento Fashion MNIST\n",
    "fmnist = tf.keras.datasets.fashion_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GuoLQQBT4E-_"
   },
   "source": [
    "Con `load_data()` possiamo ottenere due tuple ciascuna con due liste che conterranno rispettivamente i dati di training e di test etichettati (le immagini di capi di abbigliamento).\n",
    "Dividiamo il dataset in `training set` e `test set`, come visto nella teoria, per: \n",
    "* Addestrare la rete neurale sul training set, valutandone e ottimizzando automaticamente le prestazioni attraverso loss e backpropagation.\n",
    "* Testare le prestazioni effettive della rete nel riconoscimento di dati nuovi che non ha mai \"visto\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "BTdRgExe4TRB"
   },
   "outputs": [],
   "source": [
    "# Caricamento del training set e del test set a partire dal dataset Fashion MNIST\n",
    "(training_images, training_labels), (test_images, test_labels) = fmnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rw395ROx4f5Q"
   },
   "source": [
    "Priva di partire con l'addestramento, visualizziamo come è fatto uno degli esempi rappresentandolo sia con un numpy array, sia mostrando la relativa immagine. È possibile scegliere quale esempio visualizzare modificando il valore della variabile `index`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "FPc9d3gJ3jWF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL: 0\n",
      "\n",
      "IMAGE PIXEL ARRAY:\n",
      "\n",
      "[[  0   0   0   0   0   1   0   0   0   0  41 188 103  54  48  43  87 168 133  16   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   1   0   0   0  49 136 219 216 228 236 255 255 255 255 217 215 254 231 160  45   0   0   0   0   0]\n",
      " [  0   0   0   0   0  14 176 222 224 212 203 198 196 200 215 204 202 201 201 201 209 218 224 164   0   0   0   0]\n",
      " [  0   0   0   0   0 188 219 200 198 202 198 199 199 201 196 198 198 200 200 200 200 201 200 225  41   0   0   0]\n",
      " [  0   0   0   0  51 219 199 203 203 212 238 248 250 245 249 246 247 252 248 235 207 203 203 222 140   0   0   0]\n",
      " [  0   0   0   0 116 226 206 204 207 204 101  75  47  73  48  50  45  51  63 113 222 202 206 220 224   0   0   0]\n",
      " [  0   0   0   0 200 222 209 203 215 200   0  70  98   0 103  59  68  71  49   0 219 206 214 210 250  38   0   0]\n",
      " [  0   0   0   0 247 218 212 210 215 214   0 254 243 139 255 174 251 255 205   0 215 217 214 208 220  95   0   0]\n",
      " [  0   0   0  45 226 214 214 215 224 205   0  42  35  60  16  17  12  13  70   0 189 216 212 206 212 156   0   0]\n",
      " [  0   0   0 164 235 214 211 220 216 201  52  71  89  94  83  78  70  76  92  87 206 207 222 213 219 208   0   0]\n",
      " [  0   0   0 106 187 223 237 248 211 198 252 250 248 245 248 252 253 250 252 239 201 212 225 215 193 113   0   0]\n",
      " [  0   0   0   0   0  17  54 159 222 193 208 192 197 200 200 200 200 201 203 195 210 165   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  47 225 192 214 203 206 204 204 205 206 204 212 197 218 107   0   0   0   0   0   0]\n",
      " [  0   0   0   0   1   6   0  46 212 195 212 202 206 205 204 205 206 204 212 200 218  91   0   3   1   0   0   0]\n",
      " [  0   0   0   0   0   1   0  11 197 199 205 202 205 206 204 205 207 204 205 205 218  77   0   5   0   0   0   0]\n",
      " [  0   0   0   0   0   3   0   2 191 198 201 205 206 205 205 206 209 206 199 209 219  74   0   5   0   0   0   0]\n",
      " [  0   0   0   0   0   2   0   0 188 197 200 207 207 204 207 207 210 208 198 207 221  72   0   4   0   0   0   0]\n",
      " [  0   0   0   0   0   2   0   0 215 198 203 206 208 205 207 207 210 208 200 202 222  75   0   4   0   0   0   0]\n",
      " [  0   0   0   0   0   1   0   0 212 198 209 206 209 206 208 207 211 206 205 198 221  80   0   3   0   0   0   0]\n",
      " [  0   0   0   0   0   1   0   0 204 201 205 208 207 205 211 205 210 210 209 195 221  96   0   3   0   0   0   0]\n",
      " [  0   0   0   0   0   1   0   0 202 201 205 209 207 205 213 206 210 209 210 194 217 105   0   2   0   0   0   0]\n",
      " [  0   0   0   0   0   1   0   0 204 204 205 208 207 205 215 207 210 208 211 193 213 115   0   2   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 204 207 207 208 206 206 215 210 210 207 212 195 210 118   0   2   0   0   0   0]\n",
      " [  0   0   0   0   0   1   0   0 198 208 208 208 204 207 212 212 210 207 211 196 207 121   0   1   0   0   0   0]\n",
      " [  0   0   0   0   0   1   0   0 198 210 207 208 206 209 213 212 211 207 210 197 207 124   0   1   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 172 210 203 201 199 204 207 205 204 201 205 197 206 127   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 188 221 214 234 236 238 244 244 244 240 243 214 224 162   0   2   0   0   0   0]\n",
      " [  0   0   0   0   0   1   0   0 139 146 130 135 135 137 125 124 125 121 119 114 130  76   0   0   0   0   0   0]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Valori ammessi da 0 to 59999\n",
    "index = 1\n",
    "\n",
    "# Si imposta il numero di caratteri per riga in fase di stampa \n",
    "np.set_printoptions(linewidth=320)\n",
    "\n",
    "# Stampo l'indice dell'etichetta\n",
    "print(f'LABEL: {training_labels[index]}')\n",
    "# Stampo la versione Numpy dell'immagine (una matrice di numeri che rappresentano i colori)\n",
    "print(f'\\nIMAGE PIXEL ARRAY:\\n\\n{training_images[index]}\\n\\n')\n",
    "\n",
    "# Visualizzo l'immagine utilizzando la colormap predefinita (virisid)\n",
    "plt.imshow(training_images[index])\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cbrdH225_nH"
   },
   "source": [
    "I valori degli input che vengono inviati ai neuroni di input del primo strato variano da 0 a 255.\n",
    "Le reti neurali, in particolare quelle che si occupano di image processing, lavorano meglio con valori che variano tra 0 e 1.\n",
    "È opportuno quindi _normalizzare_ questi valori in modo tale che varino in un intervallo [0..1].\n",
    "Con Numpy possiamo facilmente riscalare i valori in questo modo semplicemente dividendoli per il valore massimo attuale, come di seguito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "kRH19pWs6ZDn"
   },
   "outputs": [],
   "source": [
    "# Normalizzazione dei valori dei pixel delle immagini di train e di test.\n",
    "training_images  = training_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dIn7S9gf62ie"
   },
   "source": [
    "Creazione di una MLP (Multilayer Perceptron) composta da 3 strati: \n",
    "1. 28 neuroni per lo strato di input (input layer): uno per ogni pixel dell'immagine che dovrà acquisire.\n",
    "2. 128 neuroni per lo strato nascosto (hidden layer).\n",
    "3. 10 neuroni per lo strato di outout: uno per ognuna delle tipologie di capi di abbigliamento (classi).\n",
    "\n",
    "N.B.: una rete a 3 strati non viene ancora considerata profonda (deep), perciò formalmente non siamo ancora nell'ambito del deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "7mAyndG3kVlK"
   },
   "outputs": [],
   "source": [
    "# Creazione del modello di classificazione\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.Input(shape=(28,28)),\n",
    "    tf.keras.layers.Flatten(), \n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu), \n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-lUcWaiX7MFj"
   },
   "source": [
    "Progettiamo la rete MLP (Multilayer Perceptron) a 3 strati nel dettaglio:\n",
    "\n",
    "[Sequential](https://keras.io/api/models/sequential/): Classe che definisce una rete neurale fatta a strati (MLP in questo caso).\n",
    "\n",
    "[Input](https://keras.io/api/layers/core_layers/input/): Classe che definisce lo strato di input. In questo caso la forma dell'input è 28x28 neuroni per un totale di 784 neuroni, uno per ogni pixel dell'immagine.\n",
    "\n",
    "[Flatten](https://keras.io/api/layers/reshaping_layers/flatten/): Flatten serve per \"appiattire\" l'immagine rappresentata da una matrice di valori, in un vettore mono-dimensionale. Questo processo è necessario perché le MLP normalmente non prevedono strati di neuroni multidimensionali, ma solo monodimensionali.\n",
    "Potremmo affermare che il \"flattening\" in questo caso è una semplice ma fondamentale azione di `preprocessing` che serve a preparare i dati di input per la rete.\n",
    "\n",
    "[Dense](https://keras.io/api/layers/core_layers/dense/): La classe Dense serve per crea un nuovo strato per la rete neurale. Si utilizza per creare uno strato di neuroni intermedio nascosto (`hidden layer`) nella rete MLP e il livello di uscita (`output layer`) che produce i dati in uscita.\n",
    "\n",
    "Per gli strati intermedi e per quello di output è necessario specificare una [activation function](https://keras.io/api/layers/activations/) che ciascun neurone dello strato applicherà agli input \"pesati\". Ne abbiamo viste diverse, ma in questo ne usiamo una semplice e veloce da elaborare, una [ReLU](https://keras.io/api/layers/activations/#relu-function) che fa \"passare\" solo gli input maggiori di 0 allo strato successivo:\n",
    "\n",
    "```\n",
    "if x > 0: \n",
    "  return x\n",
    "\n",
    "else: \n",
    "  return 0\n",
    "```\n",
    "Nel livello di output utilizziamo una [Softmax](https://keras.io/api/layers/activations/#softmax-function).\n",
    "La Softmax è molto comoda per questo problema di classificazione multiclasse. Prende in input un vettore di valori e lo converte in un vettore di valori compresi tra [0..1] in modo tale che la loro somma sia uguale a 1. Questi valori corrispondono alla probabilità associata ad una specifica classe e il valore più alto è quello individuato dalla rete come il più probabile associato all'immagine. Ad esempio, se il valore più alto di probabilità, in output dalla rete, si trova nell'indice 6 del vettore di uscita, allora la rete ha classificato quell'immagine, in una determinata `epoca`, come una camicia. \n",
    "\n",
    "Maggiori dettagli sul funzionamento della funziione `Softmax` nei problemi di classificazione multiclasse può essere trovato in questa lezione di `Andrew NG` sull'argomento:(https://www.youtube.com/watch?v=LLux1SW--oM&ab_channel=DeepLearningAI) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Dk1hzzpDoGPI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input to softmax function: [[1. 3. 4. 2.]]\n",
      "output of softmax function: [[0.0320586  0.23688282 0.64391426 0.08714432]]\n",
      "sum of outputs: 1.0\n",
      "class with highest probability: 2\n"
     ]
    }
   ],
   "source": [
    "# Declare sample inputs and convert to a tensor\n",
    "inputs = np.array([[1.0, 3.0, 4.0, 2.0]])\n",
    "inputs = tf.convert_to_tensor(inputs)\n",
    "print(f'input to softmax function: {inputs.numpy()}')\n",
    "\n",
    "# Feed the inputs to a softmax activation function\n",
    "outputs = tf.keras.activations.softmax(inputs)\n",
    "print(f'output of softmax function: {outputs.numpy()}')\n",
    "\n",
    "# Get the sum of all values after the softmax\n",
    "sum = tf.reduce_sum(outputs)\n",
    "print(f'sum of outputs: {sum}')\n",
    "\n",
    "# Get the index with highest value\n",
    "prediction = np.argmax(outputs)\n",
    "print(f'class with highest probability: {prediction}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c8vbMCqb9Mh6"
   },
   "source": [
    "Nel prossimo passaggio dobbiamo `compilare` il modello specificando alcuni importanti parametri: loss function da usare, ottimizzatore dei pesi della rete durante il backpropagation e metriche per valutare le prestazioni del nostro modello.\n",
    "\n",
    "La configurazione ideale di una rete neurale che risolva in maniera ottimale il nostro problema non è facile da determinare.\n",
    "Anche i parametri indicati sopra, possono essere modificati testando poi il comportamento della rete per verificarne le prestazioni.\n",
    "Qui abbiamo scelto: \n",
    "* **Loss function** [sparse_categorial_crossentropy]`tf.keras.losses.SparseCategoricalCrossentropy()`: a differenza della più classica `categorical_crossentropy`, che richiede etichette come vettori \"one hot\" (come [0, 0, 1, 0, ...]) la `sparse_categorical_crossentropy` è più comoda perché può gestire direttamente etichette numeriche come (0, 1, 2, 3, ...). Inoltre è compatibile con la `Softmax` e calcola la probabilità logaritmica per la classe corretta, minimizzando la perdita tra le probabilità predette e le etichette reali.\n",
    "* **Ottimizzatore** [Adam]`https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam`: è molto potente e versatile per i modelli di deep learning perché combina i vantaggi di SGD con quelli di RMSProp. Maggiori dettagli nella documentazione ufficiale.\n",
    "* **Accuracy**: la metrica epsrime la percentuale di esempi classificati correttamente. Risulta intuitiva nella misurazione delle prestazioni del modello. Adatta a dataset bilanciati come quello utilizzato (MNIST)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "BLMdl9aP8nQ0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 17:50:37.947402: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 1s 622us/step - loss: 0.5002 - accuracy: 0.8241\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 1s 629us/step - loss: 0.3752 - accuracy: 0.8644\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 1s 621us/step - loss: 0.3369 - accuracy: 0.8776\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 1s 594us/step - loss: 0.3121 - accuracy: 0.8870\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 1s 590us/step - loss: 0.2929 - accuracy: 0.8923\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x3104fce10>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer = tf.optimizers.Adam(),\n",
    "              loss = 'sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(training_images, training_labels, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-JJMsvSB-1UY"
   },
   "source": [
    "Durante la fase di addestramento è possibile visualizzare loss e accuracy. Già dopo sole 5 epoche è possibile ottenere un livello di accuratezza abbastanza alto, vicino a 0.9100 o superiore (91%). Questo valore significa che, alla fine dell'addestramento, il modello è in grado di riconoscere circa 91% delle immagini correttamente.\n",
    "\n",
    "Passiamo ora alla fase di test durante la quale proveremo il nostro modello su una parte del dataset che la rete non ha mai \"visto\". È importante che il test set non venga mai utilizzato nella fase di addestramento, ma solo dopo aver addestrato la rete neurale ottenendo prestazioni per noi soddisfacenti.\n",
    "\n",
    "Per lanciare la fase di test, si utilizza il metodo [`model.evaluate()`](https://keras.io/api/models/model_training_apis/#evaluate-method).\n",
    "Anche in questa fase TensorFlow ci mostrerà la loss e la accuracy ottenuta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "WzlqsEzX9s5P"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 358us/step - loss: 0.3453 - accuracy: 0.8766\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.34533563256263733, 0.8766000270843506]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model on unseen data\n",
    "model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tki-Aro_Uax"
   },
   "source": [
    "L'accuratezza, con l'addestramento effettuato, arriverà a circa 0.88 o anche di più.\n",
    "È evidente che le prestazioni non sono le stesse ottenute sul training set, quindi su esempi \"già visti\" per diverse epoche (cicli di addestramento).\n",
    "Ci sono diversi modi per migliorare le prestazioni e questa è una delle fasi più divertenti della progettazione di reti neurali, il \"fine tuning\"!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "htldZNWcIPSN"
   },
   "source": [
    "# Esercizi\n",
    "\n",
    "To explore further and deepen your understanding, try the exercises below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rquQqIx4AaGR"
   },
   "source": [
    "### Esercizio 1:\n",
    "Esegui il codice seguente che crea un set di valori di classificazione per ognuna delle immagini di test e, in seguito, stampa il primo risultato di classificazione. \n",
    "Cosa rappresentano i numeri stampati?\n",
    "\n",
    "Suggerimento: prova a eseguire `print(test_labels[0])` e otterrai il numero 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "RyEIki0z_hAD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 355us/step\n",
      "[6.3886700e-06 3.4236677e-06 1.4520826e-06 2.2417532e-06 4.3486639e-07 3.2388605e-02 1.3395985e-05 6.2088508e-02 3.1235897e-05 9.0546441e-01]\n"
     ]
    }
   ],
   "source": [
    "classifications = model.predict(test_images)\n",
    "\n",
    "print(classifications[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "WnBGOrMiA1n5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "print(test_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OgQSIfDSOWv6"
   },
   "source": [
    "### Esercizio 2: \n",
    "Proviamo a modificare un solo parametro del nostro modello: il numero di neuroni dello strato \"hidden\". Usiamo 512 neuroni.\n",
    "Quali risultati otteniamo in termini di: \n",
    "* Loss\n",
    "* Accuracy\n",
    "* Training time\n",
    "\n",
    "Se hai la possibilità, esegui il codice su computer o cloud con diverse caratteristiche, soprattutto in termini di CPU e GPU per valutare i tempi di esecuzione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "GSZSwV5UObQP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n",
      "\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 1s 616us/step - loss: 0.4977 - accuracy: 0.8245\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 1s 606us/step - loss: 0.3736 - accuracy: 0.8651\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 1s 591us/step - loss: 0.3374 - accuracy: 0.8770\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 1s 594us/step - loss: 0.3124 - accuracy: 0.8860\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 1s 596us/step - loss: 0.2949 - accuracy: 0.8920\n",
      "\n",
      "Evaluating on test set:\n",
      "\n",
      "313/313 [==============================] - 0s 394us/step - loss: 0.3554 - accuracy: 0.8725\n",
      "\n",
      "Predicting using test set:\n",
      "\n",
      "313/313 [==============================] - 0s 339us/step\n",
      "\n",
      "True class for first image on test set: 9\n",
      "Probability of each class:\n",
      "[8.9833075e-06 4.7638824e-08 2.1215430e-07 7.9688744e-09 2.3188512e-07 8.1178173e-04 2.9016965e-06 1.9008942e-02 4.0093182e-05 9.8012674e-01]\n",
      "\n",
      "Total processing time: 6.14 seconds\n"
     ]
    }
   ],
   "source": [
    "fmnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(training_images, training_labels), (test_images, test_labels) = fmnist.load_data()\n",
    "\n",
    "training_images = training_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.Input(shape=(28,28)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu), # Try experimenting with this layer\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Training:\\n\")\n",
    "model.fit(training_images, training_labels, epochs=5)\n",
    "\n",
    "print(\"\\nEvaluating on test set:\\n\")\n",
    "model.evaluate(test_images, test_labels)\n",
    "\n",
    "print(\"\\nPredicting using test set:\\n\")\n",
    "predictions = model.predict(test_images)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"\\nTrue class for first image on test set: {test_labels[0]}\\nProbability of each class:\\n{predictions[0]}\")\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"\\nTotal processing time: {elapsed_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bOOEnHZFv5cS"
   },
   "source": [
    "### Esercizio 2.1: Aumenta a 1024 neuroni il livello \"hidden\" -- Cosa cambia?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ExNxCwhcQ18S"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n",
      "\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 1s 603us/step - loss: 0.4957 - accuracy: 0.8244\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 1s 593us/step - loss: 0.3740 - accuracy: 0.8658\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 1s 571us/step - loss: 0.3373 - accuracy: 0.8765\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 1s 599us/step - loss: 0.3118 - accuracy: 0.8846\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 1s 612us/step - loss: 0.2940 - accuracy: 0.8909\n",
      "\n",
      "Evaluating on test set:\n",
      "\n",
      "313/313 [==============================] - 0s 397us/step - loss: 0.3597 - accuracy: 0.8668\n",
      "\n",
      "Predicting using test set:\n",
      "\n",
      "313/313 [==============================] - 0s 340us/step\n",
      "\n",
      "True class for first image on test set: 9\n",
      "Probability of each class:\n",
      "[2.1473121e-05 5.1850102e-10 1.0713606e-08 1.9264870e-09 4.3422851e-08 1.6556004e-03 7.3024927e-07 6.4514138e-02 6.0949396e-06 9.3380195e-01]\n",
      "\n",
      "Total processing time: 6.19 seconds\n"
     ]
    }
   ],
   "source": [
    "fmnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(training_images, training_labels), (test_images, test_labels) = fmnist.load_data()\n",
    "\n",
    "training_images = training_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.Input(shape=(28,28)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu), # Prova a usare 1024 neuroni\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Training:\\n\")\n",
    "model.fit(training_images, training_labels, epochs=5)\n",
    "\n",
    "print(\"\\nEvaluating on test set:\\n\")\n",
    "model.evaluate(test_images, test_labels)\n",
    "\n",
    "print(\"\\nPredicting using test set:\\n\")\n",
    "predictions = model.predict(test_images)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"\\nTrue class for first image on test set: {test_labels[0]}\\nProbability of each class:\\n{predictions[0]}\")\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"\\nTotal processing time: {elapsed_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-0lF5MuvSuZF"
   },
   "source": [
    "### Esercizio 3: \n",
    "Cosa succede se aggiungo dei livelli intermedi nella rete neurale realizzando quindi una \"deep neural network\", una rete neurale profonda?\n",
    "\n",
    "Prova a inserire uno o più livelli nascosti (\"hidden layer\") e valutane gli effetti.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "b1YPa6UhS8Es"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n",
      "\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 2s 788us/step - loss: 5.6003 - accuracy: 0.1576\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 1s 752us/step - loss: 5.5047 - accuracy: 0.1111\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 2s 813us/step - loss: 5.4682 - accuracy: 0.1140\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 1s 745us/step - loss: 5.4627 - accuracy: 0.1149\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 1s 735us/step - loss: 5.4681 - accuracy: 0.1152\n",
      "\n",
      "Evaluating on test set:\n",
      "\n",
      "313/313 [==============================] - 0s 471us/step - loss: 5.4702 - accuracy: 0.1147\n",
      "\n",
      "Predicting using test set:\n",
      "\n",
      "313/313 [==============================] - 0s 361us/step\n",
      "\n",
      "True class for first image on test set: 9\n",
      "Probability of each class:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Total processing time: 7.70 seconds\n"
     ]
    }
   ],
   "source": [
    "fmnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(training_images, training_labels), (test_images, test_labels) = fmnist.load_data()\n",
    "\n",
    "training_images = training_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.Input(shape=(28, 28)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    # Aggiungi un layer,\n",
    "    tf.keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "    # Aggiungi un layer\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Training:\\n\")\n",
    "model.fit(training_images, training_labels, epochs=5)\n",
    "\n",
    "print(\"\\nEvaluating on test set:\\n\")\n",
    "model.evaluate(test_images, test_labels)\n",
    "\n",
    "print(\"\\nPredicting using test set:\\n\")\n",
    "predictions = model.predict(test_images)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"\\nTrue class for first image on test set: {test_labels[0]}\\nProbability of each class:\\n{predictions[0]}\")\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"\\nTotal processing time: {elapsed_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bql9fyaNUSFy"
   },
   "source": [
    "### Esercizio 4: \n",
    "\n",
    "### Valuta l'impatto di un addestramento per più o meno epoche. Cosa succede?\n",
    "\n",
    "- Con 15 epoche: otterrai probabilmente una loss migliore rispetto a usare 5 epoche.\n",
    "- Con 30 o più epoche: probabilmente la loss calerà più lentamente e, a volte, aumenterà. Inoltre i risultati sul test set non miglioreranno significativamente e potrebbero peggiorare in alcuni casi. \n",
    "\n",
    "L'addestramento per un numero di epoche più elevato del necessario può portare a `overfitting` (sovrallenamento). \n",
    "Se la loss non migliora in maniera significativamente, non ha senso continuare investire più tempo nell'addestramento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "uE3esj9BURQe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n",
      "\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 1s 640us/step - loss: 0.4939 - accuracy: 0.8268\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 1s 605us/step - loss: 0.3727 - accuracy: 0.8651\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 1s 605us/step - loss: 0.3353 - accuracy: 0.8779\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 1s 609us/step - loss: 0.3091 - accuracy: 0.8872\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 1s 611us/step - loss: 0.2933 - accuracy: 0.8916\n",
      "\n",
      "Evaluating on test set:\n",
      "\n",
      "313/313 [==============================] - 0s 380us/step - loss: 0.3618 - accuracy: 0.8706\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.36177700757980347, 0.8705999851226807]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(training_images, training_labels), (test_images, test_labels) = fmnist.load_data()\n",
    "\n",
    "training_images = training_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.Input(shape=(28,28)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu), # Try experimenting with this layer\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Training:\\n\")\n",
    "model.fit(training_images, training_labels, epochs=5)\n",
    "\n",
    "print(\"\\nEvaluating on test set:\\n\")\n",
    "model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HS3vVkOgCDGZ"
   },
   "source": [
    "### Esercizio 5: \n",
    "\n",
    "Prima di addestrare la rete abbiamo normalizzato i dati a [0..1] partendo da valori nell'intervallo [0..255].\n",
    "Cosa succede se non facciamo la normalizzazione? Perché ottengo risultati diversi?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "JDqNAqrpCNg0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n",
      "\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.4723 - accuracy: 0.8321\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.3599 - accuracy: 0.8691\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.3249 - accuracy: 0.8814\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.2993 - accuracy: 0.8894\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.2802 - accuracy: 0.8960\n",
      "\n",
      "Evaluating on test set:\n",
      "\n",
      "313/313 [==============================] - 0s 574us/step - loss: 0.3738 - accuracy: 0.8652\n",
      "\n",
      "Predicting using test set:\n",
      "\n",
      "313/313 [==============================] - 0s 485us/step\n",
      "\n",
      "True class for first image on test set: 9\n",
      "Probability of each class:\n",
      "[9.0807356e-07 4.8584776e-09 2.5810673e-08 3.4335583e-09 9.7554373e-09 9.1390153e-03 5.8926592e-08 3.1724370e-03 7.2427461e-08 9.8768741e-01]\n"
     ]
    }
   ],
   "source": [
    "fmnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(training_images, training_labels), (test_images, test_labels) = fmnist.load_data()\n",
    "\n",
    "\n",
    "training_images=training_images / 255.0 # Experiment with removing this line\n",
    "test_images=test_images / 255.0 # Experiment with removing this line\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.Input(shape=(28,28)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation=tf.nn.relu), # Try experimenting with this layer\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Training:\\n\")\n",
    "model.fit(training_images, training_labels, epochs=5)\n",
    "\n",
    "print(\"\\nEvaluating on test set:\\n\")\n",
    "model.evaluate(test_images, test_labels)\n",
    "\n",
    "print(\"\\nPredicting using test set:\\n\")\n",
    "predictions = model.predict(test_images)\n",
    "\n",
    "print(f\"\\nTrue class for first image on test set: {test_labels[0]}\\nProbability of each class:\\n{predictions[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7W2PT66ZBHQ"
   },
   "source": [
    "### Esercizio 6: \n",
    "Se addestriamo il modello per un numero di epoche superiore al necessario possiamo, nella migliore delle ipotesi, non ottenere prestazioni migliori in termini di loss e accuratezza. Nel peggiore dei casi le prestazioni possono anche degenerare portando a valori di loss occasionalmente più alti e di accuratezza più bassi.\n",
    "\n",
    "Con TensorFlow è possibile fermare l'addestramento una volta raggiunto il livello di accuratezza desiderato.\n",
    "Questa possibilità viene denominata `early stopping` e permette di fermare l'addestramento in determinate condizioni.\n",
    "\n",
    "Il meccanismo tramite il quale è possibile implementarlo è quello delle `callback`, funzioni che posso invocare durante l'addestramento per gestirlo meglio. \n",
    "\n",
    "Prova il meccanismo delle callback per fermare l'addestramento quando si verificano le condizioni che desideri."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "pkaEHHgqZbYv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1838/1875 [============================>.] - ETA: 0s - loss: 0.4735 - accuracy: 0.8327\n",
      "Reached 60% accuracy so cancelling training!\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.4710 - accuracy: 0.8336\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x37ecd1350>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if(logs.get('accuracy') >= 0.6): # Experiment with changing this value\n",
    "            print(\"\\nReached 60% accuracy so cancelling training!\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "callbacks = myCallback()\n",
    "\n",
    "fmnist = tf.keras.datasets.fashion_mnist\n",
    "(training_images, training_labels), (test_images, test_labels) = fmnist.load_data()\n",
    "\n",
    "training_images=training_images / 255.0\n",
    "test_images=test_images / 255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.Input(shape=(28,28)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation=tf.nn.relu), # Try experimenting with this layer\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(training_images, training_labels, epochs=5, callbacks=[callbacks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "C1_W2_Lab_1_beyond_hello_world.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "bc58f1a9918615c43466b117602939cc46a8cba292d69906d63eff60c7bc7f26"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
